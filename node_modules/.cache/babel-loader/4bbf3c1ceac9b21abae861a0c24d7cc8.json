{"ast":null,"code":"'use strict';\n\nvar _regeneratorRuntime = require(\"C:/Users/zajan/GitHub/chatApplication/node_modules/@babel/runtime/helpers/regeneratorRuntime.js\").default;\n\nvar _asyncToGenerator = require(\"C:/Users/zajan/GitHub/chatApplication/node_modules/@babel/runtime/helpers/asyncToGenerator.js\").default;\n\nvar _classCallCheck = require(\"C:/Users/zajan/GitHub/chatApplication/node_modules/@babel/runtime/helpers/classCallCheck.js\").default;\n\nvar _createClass = require(\"C:/Users/zajan/GitHub/chatApplication/node_modules/@babel/runtime/helpers/createClass.js\").default;\n\nvar Entry = require('./entry');\n\nvar EntryIO = require('./entry-io');\n\nvar Sorting = require('./log-sorting');\n\nvar LastWriteWins = Sorting.LastWriteWins,\n    NoZeroes = Sorting.NoZeroes;\n\nvar LogError = require('./log-errors');\n\nvar _require = require('./utils'),\n    isDefined = _require.isDefined,\n    findUniques = _require.findUniques,\n    difference = _require.difference,\n    io = _require.io;\n\nvar IPLD_LINKS = ['heads'];\n\nvar last = function last(arr, n) {\n  return arr.slice(arr.length - Math.min(arr.length, n), arr.length);\n};\n\nvar LogIO = /*#__PURE__*/function () {\n  function LogIO() {\n    _classCallCheck(this, LogIO);\n  }\n\n  _createClass(LogIO, null, [{\n    key: \"toMultihash\",\n    value: //\n\n    /**\n     * Get the multihash of a Log.\n     * @param {IPFS} ipfs An IPFS instance\n     * @param {Log} log Log to get a multihash for\n     * @returns {Promise<string>}\n     * @deprecated\n     */\n    function () {\n      var _toMultihash = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(ipfs, log) {\n        var _ref,\n            format,\n            _args = arguments;\n\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                _ref = _args.length > 2 && _args[2] !== undefined ? _args[2] : {}, format = _ref.format;\n\n                if (isDefined(ipfs)) {\n                  _context.next = 3;\n                  break;\n                }\n\n                throw LogError.IPFSNotDefinedError();\n\n              case 3:\n                if (isDefined(log)) {\n                  _context.next = 5;\n                  break;\n                }\n\n                throw LogError.LogNotDefinedError();\n\n              case 5:\n                if (!isDefined(format)) format = 'dag-cbor';\n\n                if (!(log.values.length < 1)) {\n                  _context.next = 8;\n                  break;\n                }\n\n                throw new Error('Can\\'t serialize an empty log');\n\n              case 8:\n                return _context.abrupt(\"return\", io.write(ipfs, format, log.toJSON(), {\n                  links: IPLD_LINKS\n                }));\n\n              case 9:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee);\n      }));\n\n      function toMultihash(_x, _x2) {\n        return _toMultihash.apply(this, arguments);\n      }\n\n      return toMultihash;\n    }()\n    /**\n     * Create a log from a hashes.\n     * @param {IPFS} ipfs An IPFS instance\n     * @param {string} hash The hash of the log\n     * @param {Object} options\n     * @param {number} options.length How many items to include in the log\n     * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n     * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n     */\n\n  }, {\n    key: \"fromMultihash\",\n    value: function () {\n      var _fromMultihash = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(ipfs, hash, _ref2) {\n        var _ref2$length, length, _ref2$exclude, exclude, shouldExclude, timeout, concurrency, sortFn, onProgressCallback, logData, isHead, all, logId, entries, heads;\n\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                _ref2$length = _ref2.length, length = _ref2$length === void 0 ? -1 : _ref2$length, _ref2$exclude = _ref2.exclude, exclude = _ref2$exclude === void 0 ? [] : _ref2$exclude, shouldExclude = _ref2.shouldExclude, timeout = _ref2.timeout, concurrency = _ref2.concurrency, sortFn = _ref2.sortFn, onProgressCallback = _ref2.onProgressCallback;\n\n                if (isDefined(ipfs)) {\n                  _context2.next = 3;\n                  break;\n                }\n\n                throw LogError.IPFSNotDefinedError();\n\n              case 3:\n                if (isDefined(hash)) {\n                  _context2.next = 5;\n                  break;\n                }\n\n                throw new Error(\"Invalid hash: \".concat(hash));\n\n              case 5:\n                _context2.next = 7;\n                return io.read(ipfs, hash, {\n                  links: IPLD_LINKS\n                });\n\n              case 7:\n                logData = _context2.sent;\n\n                if (!(!logData.heads || !logData.id)) {\n                  _context2.next = 10;\n                  break;\n                }\n\n                throw LogError.NotALogError();\n\n              case 10:\n                // Use user provided sorting function or the default one\n                sortFn = sortFn || NoZeroes(LastWriteWins);\n\n                isHead = function isHead(e) {\n                  return logData.heads.includes(e.hash);\n                };\n\n                _context2.next = 14;\n                return EntryIO.fetchAll(ipfs, logData.heads, {\n                  length: length,\n                  exclude: exclude,\n                  shouldExclude: shouldExclude,\n                  timeout: timeout,\n                  concurrency: concurrency,\n                  onProgressCallback: onProgressCallback\n                });\n\n              case 14:\n                all = _context2.sent;\n                logId = logData.id;\n                entries = length > -1 ? last(all.sort(sortFn), length) : all;\n                heads = entries.filter(isHead);\n                return _context2.abrupt(\"return\", {\n                  logId: logId,\n                  entries: entries,\n                  heads: heads\n                });\n\n              case 19:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2);\n      }));\n\n      function fromMultihash(_x3, _x4, _x5) {\n        return _fromMultihash.apply(this, arguments);\n      }\n\n      return fromMultihash;\n    }()\n    /**\n     * Create a log from an entry hash.\n     * @param {IPFS} ipfs An IPFS instance\n     * @param {string} hash The hash of the entry\n     * @param {Object} options\n     * @param {number} options.length How many items to include in the log\n     * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n     * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n     */\n\n  }, {\n    key: \"fromEntryHash\",\n    value: function () {\n      var _fromEntryHash = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3(ipfs, hash, _ref3) {\n        var _ref3$length, length, _ref3$exclude, exclude, shouldExclude, timeout, concurrency, sortFn, onProgressCallback, hashes, all, entries;\n\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                _ref3$length = _ref3.length, length = _ref3$length === void 0 ? -1 : _ref3$length, _ref3$exclude = _ref3.exclude, exclude = _ref3$exclude === void 0 ? [] : _ref3$exclude, shouldExclude = _ref3.shouldExclude, timeout = _ref3.timeout, concurrency = _ref3.concurrency, sortFn = _ref3.sortFn, onProgressCallback = _ref3.onProgressCallback;\n\n                if (isDefined(ipfs)) {\n                  _context3.next = 3;\n                  break;\n                }\n\n                throw LogError.IpfsNotDefinedError();\n\n              case 3:\n                if (isDefined(hash)) {\n                  _context3.next = 5;\n                  break;\n                }\n\n                throw new Error(\"'hash' must be defined\");\n\n              case 5:\n                // Convert input hash(s) to an array\n                hashes = Array.isArray(hash) ? hash : [hash]; // Fetch given length, return size at least the given input entries\n\n                length = length > -1 ? Math.max(length, 1) : length;\n                _context3.next = 9;\n                return EntryIO.fetchParallel(ipfs, hashes, {\n                  length: length,\n                  exclude: exclude,\n                  shouldExclude: shouldExclude,\n                  timeout: timeout,\n                  concurrency: concurrency,\n                  onProgressCallback: onProgressCallback\n                });\n\n              case 9:\n                all = _context3.sent;\n                // Cap the result at the right size by taking the last n entries,\n                // or if given length is -1, then take all\n                sortFn = sortFn || NoZeroes(LastWriteWins);\n                entries = length > -1 ? last(all.sort(sortFn), length) : all;\n                return _context3.abrupt(\"return\", {\n                  entries: entries\n                });\n\n              case 13:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, _callee3);\n      }));\n\n      function fromEntryHash(_x6, _x7, _x8) {\n        return _fromEntryHash.apply(this, arguments);\n      }\n\n      return fromEntryHash;\n    }()\n    /**\n     * Creates a log data from a JSON object, to be passed to a Log constructor\n     *\n     * @param {IPFS} ipfs An IPFS instance\n     * @param {json} json A json object containing valid log data\n     * @param {Object} options\n     * @param {number} options.length How many entries to include\n     * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n     **/\n\n  }, {\n    key: \"fromJSON\",\n    value: function () {\n      var _fromJSON = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(ipfs, json, _ref4) {\n        var _ref4$length, length, timeout, concurrency, onProgressCallback, id, heads, headHashes, all, entries;\n\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) {\n            switch (_context4.prev = _context4.next) {\n              case 0:\n                _ref4$length = _ref4.length, length = _ref4$length === void 0 ? -1 : _ref4$length, timeout = _ref4.timeout, concurrency = _ref4.concurrency, onProgressCallback = _ref4.onProgressCallback;\n\n                if (isDefined(ipfs)) {\n                  _context4.next = 3;\n                  break;\n                }\n\n                throw LogError.IPFSNotDefinedError();\n\n              case 3:\n                id = json.id, heads = json.heads;\n                headHashes = heads.map(function (e) {\n                  return e.hash;\n                });\n                _context4.next = 7;\n                return EntryIO.fetchParallel(ipfs, headHashes, {\n                  length: length,\n                  timeout: timeout,\n                  concurrency: concurrency,\n                  onProgressCallback: onProgressCallback\n                });\n\n              case 7:\n                all = _context4.sent;\n                entries = all.sort(Entry.compare);\n                return _context4.abrupt(\"return\", {\n                  logId: id,\n                  entries: entries,\n                  heads: heads\n                });\n\n              case 10:\n              case \"end\":\n                return _context4.stop();\n            }\n          }\n        }, _callee4);\n      }));\n\n      function fromJSON(_x9, _x10, _x11) {\n        return _fromJSON.apply(this, arguments);\n      }\n\n      return fromJSON;\n    }()\n    /**\n     * Create a new log starting from an entry.\n     * @param {IPFS} ipfs An IPFS instance\n     * @param {Entry|Array<Entry>} sourceEntries An entry or an array of entries to fetch a log from\n     * @param {Object} options\n     * @param {number} options.length How many entries to include\n     * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n     * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n     */\n\n  }, {\n    key: \"fromEntry\",\n    value: function () {\n      var _fromEntry = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee5(ipfs, sourceEntries, _ref5) {\n        var _ref5$length, length, _ref5$exclude, exclude, shouldExclude, timeout, concurrency, onProgressCallback, hashes, all, combined, uniques, sliced, missingSourceEntries, replaceInFront, entries, logId;\n\n        return _regeneratorRuntime().wrap(function _callee5$(_context5) {\n          while (1) {\n            switch (_context5.prev = _context5.next) {\n              case 0:\n                _ref5$length = _ref5.length, length = _ref5$length === void 0 ? -1 : _ref5$length, _ref5$exclude = _ref5.exclude, exclude = _ref5$exclude === void 0 ? [] : _ref5$exclude, shouldExclude = _ref5.shouldExclude, timeout = _ref5.timeout, concurrency = _ref5.concurrency, onProgressCallback = _ref5.onProgressCallback;\n\n                if (isDefined(ipfs)) {\n                  _context5.next = 3;\n                  break;\n                }\n\n                throw LogError.IPFSNotDefinedError();\n\n              case 3:\n                if (isDefined(sourceEntries)) {\n                  _context5.next = 5;\n                  break;\n                }\n\n                throw new Error(\"'sourceEntries' must be defined\");\n\n              case 5:\n                if (!(!Array.isArray(sourceEntries) && !Entry.isEntry(sourceEntries))) {\n                  _context5.next = 7;\n                  break;\n                }\n\n                throw new Error('\\'sourceEntries\\' argument must be an array of Entry instances or a single Entry');\n\n              case 7:\n                if (!Array.isArray(sourceEntries)) {\n                  sourceEntries = [sourceEntries];\n                } // Fetch given length, return size at least the given input entries\n\n\n                length = length > -1 ? Math.max(length, sourceEntries.length) : length; // Make sure we pass hashes instead of objects to the fetcher function\n\n                hashes = sourceEntries.map(function (e) {\n                  return e.hash;\n                }); // Fetch the entries\n\n                _context5.next = 12;\n                return EntryIO.fetchParallel(ipfs, hashes, {\n                  length: length,\n                  exclude: exclude,\n                  shouldExclude: shouldExclude,\n                  timeout: timeout,\n                  concurrency: concurrency,\n                  onProgressCallback: onProgressCallback\n                });\n\n              case 12:\n                all = _context5.sent;\n                // Combine the fetches with the source entries and take only uniques\n                combined = sourceEntries.concat(all).concat(exclude);\n                uniques = findUniques(combined, 'hash').sort(Entry.compare); // Cap the result at the right size by taking the last n entries\n\n                sliced = uniques.slice(length > -1 ? -length : -uniques.length); // Make sure that the given input entries are present in the result\n                // in order to not lose references\n\n                missingSourceEntries = difference(sliced, sourceEntries, 'hash');\n\n                replaceInFront = function replaceInFront(a, withEntries) {\n                  var sliced = a.slice(withEntries.length, a.length);\n                  return withEntries.concat(sliced);\n                }; // Add the input entries at the beginning of the array and remove\n                // as many elements from the array before inserting the original entries\n\n\n                entries = replaceInFront(sliced, missingSourceEntries);\n                logId = entries[entries.length - 1].id;\n                return _context5.abrupt(\"return\", {\n                  logId: logId,\n                  entries: entries\n                });\n\n              case 21:\n              case \"end\":\n                return _context5.stop();\n            }\n          }\n        }, _callee5);\n      }));\n\n      function fromEntry(_x12, _x13, _x14) {\n        return _fromEntry.apply(this, arguments);\n      }\n\n      return fromEntry;\n    }()\n  }]);\n\n  return LogIO;\n}();\n\nmodule.exports = LogIO;","map":{"version":3,"names":["Entry","require","EntryIO","Sorting","LastWriteWins","NoZeroes","LogError","isDefined","findUniques","difference","io","IPLD_LINKS","last","arr","n","slice","length","Math","min","LogIO","ipfs","log","format","IPFSNotDefinedError","LogNotDefinedError","values","Error","write","toJSON","links","hash","exclude","shouldExclude","timeout","concurrency","sortFn","onProgressCallback","read","logData","heads","id","NotALogError","isHead","e","includes","fetchAll","all","logId","entries","sort","filter","IpfsNotDefinedError","hashes","Array","isArray","max","fetchParallel","json","headHashes","map","compare","sourceEntries","isEntry","combined","concat","uniques","sliced","missingSourceEntries","replaceInFront","a","withEntries","module","exports"],"sources":["C:/Users/zajan/GitHub/chatApplication/node_modules/ipfs-log/src/log-io.js"],"sourcesContent":["'use strict'\n\nconst Entry = require('./entry')\nconst EntryIO = require('./entry-io')\nconst Sorting = require('./log-sorting')\nconst { LastWriteWins, NoZeroes } = Sorting\nconst LogError = require('./log-errors')\nconst { isDefined, findUniques, difference, io } = require('./utils')\n\nconst IPLD_LINKS = ['heads']\nconst last = (arr, n) => arr.slice(arr.length - Math.min(arr.length, n), arr.length)\n\nclass LogIO {\n  //\n  /**\n   * Get the multihash of a Log.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Log} log Log to get a multihash for\n   * @returns {Promise<string>}\n   * @deprecated\n   */\n  static async toMultihash (ipfs, log, { format } = {}) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError()\n    if (!isDefined(log)) throw LogError.LogNotDefinedError()\n    if (!isDefined(format)) format = 'dag-cbor'\n    if (log.values.length < 1) throw new Error('Can\\'t serialize an empty log')\n\n    return io.write(ipfs, format, log.toJSON(), { links: IPLD_LINKS })\n  }\n\n  /**\n   * Create a log from a hashes.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {string} hash The hash of the log\n   * @param {Object} options\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n  static async fromMultihash (ipfs, hash,\n    { length = -1, exclude = [], shouldExclude, timeout, concurrency, sortFn, onProgressCallback }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError()\n    if (!isDefined(hash)) throw new Error(`Invalid hash: ${hash}`)\n\n    const logData = await io.read(ipfs, hash, { links: IPLD_LINKS })\n\n    if (!logData.heads || !logData.id) throw LogError.NotALogError()\n\n    // Use user provided sorting function or the default one\n    sortFn = sortFn || NoZeroes(LastWriteWins)\n    const isHead = e => logData.heads.includes(e.hash)\n\n    const all = await EntryIO.fetchAll(ipfs, logData.heads,\n      { length, exclude, shouldExclude, timeout, concurrency, onProgressCallback })\n\n    const logId = logData.id\n    const entries = length > -1 ? last(all.sort(sortFn), length) : all\n    const heads = entries.filter(isHead)\n    return { logId, entries, heads }\n  }\n\n  /**\n   * Create a log from an entry hash.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {string} hash The hash of the entry\n   * @param {Object} options\n   * @param {number} options.length How many items to include in the log\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n  static async fromEntryHash (ipfs, hash,\n    { length = -1, exclude = [], shouldExclude, timeout, concurrency, sortFn, onProgressCallback }) {\n    if (!isDefined(ipfs)) throw LogError.IpfsNotDefinedError()\n    if (!isDefined(hash)) throw new Error(\"'hash' must be defined\")\n    // Convert input hash(s) to an array\n    const hashes = Array.isArray(hash) ? hash : [hash]\n    // Fetch given length, return size at least the given input entries\n    length = length > -1 ? Math.max(length, 1) : length\n    const all = await EntryIO.fetchParallel(ipfs, hashes,\n      { length, exclude, shouldExclude, timeout, concurrency, onProgressCallback })\n    // Cap the result at the right size by taking the last n entries,\n    // or if given length is -1, then take all\n    sortFn = sortFn || NoZeroes(LastWriteWins)\n    const entries = length > -1 ? last(all.sort(sortFn), length) : all\n    return { entries }\n  }\n\n  /**\n   * Creates a log data from a JSON object, to be passed to a Log constructor\n   *\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {json} json A json object containing valid log data\n   * @param {Object} options\n   * @param {number} options.length How many entries to include\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   **/\n  static async fromJSON (ipfs, json, { length = -1, timeout, concurrency, onProgressCallback }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError()\n    const { id, heads } = json\n    const headHashes = heads.map(e => e.hash)\n    const all = await EntryIO.fetchParallel(ipfs, headHashes,\n      { length, timeout, concurrency, onProgressCallback })\n    const entries = all.sort(Entry.compare)\n    return { logId: id, entries, heads }\n  }\n\n  /**\n   * Create a new log starting from an entry.\n   * @param {IPFS} ipfs An IPFS instance\n   * @param {Entry|Array<Entry>} sourceEntries An entry or an array of entries to fetch a log from\n   * @param {Object} options\n   * @param {number} options.length How many entries to include\n   * @param {Array<Entry>} options.exclude Entries to not fetch (cached)\n   * @param {function(hash, entry, parent, depth)} options.onProgressCallback\n   */\n  static async fromEntry (ipfs, sourceEntries,\n    { length = -1, exclude = [], shouldExclude, timeout, concurrency, onProgressCallback }) {\n    if (!isDefined(ipfs)) throw LogError.IPFSNotDefinedError()\n    if (!isDefined(sourceEntries)) throw new Error(\"'sourceEntries' must be defined\")\n\n    // Make sure we only have Entry objects as input\n    if (!Array.isArray(sourceEntries) && !Entry.isEntry(sourceEntries)) {\n      throw new Error('\\'sourceEntries\\' argument must be an array of Entry instances or a single Entry')\n    }\n\n    if (!Array.isArray(sourceEntries)) {\n      sourceEntries = [sourceEntries]\n    }\n\n    // Fetch given length, return size at least the given input entries\n    length = length > -1 ? Math.max(length, sourceEntries.length) : length\n\n    // Make sure we pass hashes instead of objects to the fetcher function\n    const hashes = sourceEntries.map(e => e.hash)\n\n    // Fetch the entries\n    const all = await EntryIO.fetchParallel(ipfs, hashes,\n      { length, exclude, shouldExclude, timeout, concurrency, onProgressCallback })\n\n    // Combine the fetches with the source entries and take only uniques\n    const combined = sourceEntries.concat(all).concat(exclude)\n    const uniques = findUniques(combined, 'hash').sort(Entry.compare)\n\n    // Cap the result at the right size by taking the last n entries\n    const sliced = uniques.slice(length > -1 ? -length : -uniques.length)\n\n    // Make sure that the given input entries are present in the result\n    // in order to not lose references\n    const missingSourceEntries = difference(sliced, sourceEntries, 'hash')\n\n    const replaceInFront = (a, withEntries) => {\n      const sliced = a.slice(withEntries.length, a.length)\n      return withEntries.concat(sliced)\n    }\n\n    // Add the input entries at the beginning of the array and remove\n    // as many elements from the array before inserting the original entries\n    const entries = replaceInFront(sliced, missingSourceEntries)\n    const logId = entries[entries.length - 1].id\n    return { logId, entries }\n  }\n}\n\nmodule.exports = LogIO\n"],"mappings":"AAAA;;;;;;;;;;AAEA,IAAMA,KAAK,GAAGC,OAAO,CAAC,SAAD,CAArB;;AACA,IAAMC,OAAO,GAAGD,OAAO,CAAC,YAAD,CAAvB;;AACA,IAAME,OAAO,GAAGF,OAAO,CAAC,eAAD,CAAvB;;AACA,IAAQG,aAAR,GAAoCD,OAApC,CAAQC,aAAR;AAAA,IAAuBC,QAAvB,GAAoCF,OAApC,CAAuBE,QAAvB;;AACA,IAAMC,QAAQ,GAAGL,OAAO,CAAC,cAAD,CAAxB;;AACA,eAAmDA,OAAO,CAAC,SAAD,CAA1D;AAAA,IAAQM,SAAR,YAAQA,SAAR;AAAA,IAAmBC,WAAnB,YAAmBA,WAAnB;AAAA,IAAgCC,UAAhC,YAAgCA,UAAhC;AAAA,IAA4CC,EAA5C,YAA4CA,EAA5C;;AAEA,IAAMC,UAAU,GAAG,CAAC,OAAD,CAAnB;;AACA,IAAMC,IAAI,GAAG,SAAPA,IAAO,CAACC,GAAD,EAAMC,CAAN;EAAA,OAAYD,GAAG,CAACE,KAAJ,CAAUF,GAAG,CAACG,MAAJ,GAAaC,IAAI,CAACC,GAAL,CAASL,GAAG,CAACG,MAAb,EAAqBF,CAArB,CAAvB,EAAgDD,GAAG,CAACG,MAApD,CAAZ;AAAA,CAAb;;IAEMG,K;;;;;;;WACJ;;IACA;AACF;AACA;AACA;AACA;AACA;AACA;;oFACE,iBAA0BC,IAA1B,EAAgCC,GAAhC;QAAA;QAAA;QAAA;;QAAA;UAAA;YAAA;cAAA;gBAAA,+DAAkD,EAAlD,EAAuCC,MAAvC,QAAuCA,MAAvC;;gBAAA,IACOf,SAAS,CAACa,IAAD,CADhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAC8Bd,QAAQ,CAACiB,mBAAT,EAD9B;;cAAA;gBAAA,IAEOhB,SAAS,CAACc,GAAD,CAFhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAE6Bf,QAAQ,CAACkB,kBAAT,EAF7B;;cAAA;gBAGE,IAAI,CAACjB,SAAS,CAACe,MAAD,CAAd,EAAwBA,MAAM,GAAG,UAAT;;gBAH1B,MAIMD,GAAG,CAACI,MAAJ,CAAWT,MAAX,GAAoB,CAJ1B;kBAAA;kBAAA;gBAAA;;gBAAA,MAImC,IAAIU,KAAJ,CAAU,+BAAV,CAJnC;;cAAA;gBAAA,iCAMShB,EAAE,CAACiB,KAAH,CAASP,IAAT,EAAeE,MAAf,EAAuBD,GAAG,CAACO,MAAJ,EAAvB,EAAqC;kBAAEC,KAAK,EAAElB;gBAAT,CAArC,CANT;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,C;;;;;;;;IASA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;sFACE,kBAA4BS,IAA5B,EAAkCU,IAAlC;QAAA;;QAAA;UAAA;YAAA;cAAA;gBAAA,qBACId,MADJ,EACIA,MADJ,6BACa,CAAC,CADd,uCACiBe,OADjB,EACiBA,OADjB,8BAC2B,EAD3B,kBAC+BC,aAD/B,SAC+BA,aAD/B,EAC8CC,OAD9C,SAC8CA,OAD9C,EACuDC,WADvD,SACuDA,WADvD,EACoEC,MADpE,SACoEA,MADpE,EAC4EC,kBAD5E,SAC4EA,kBAD5E;;gBAAA,IAEO7B,SAAS,CAACa,IAAD,CAFhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAE8Bd,QAAQ,CAACiB,mBAAT,EAF9B;;cAAA;gBAAA,IAGOhB,SAAS,CAACuB,IAAD,CAHhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAG8B,IAAIJ,KAAJ,yBAA2BI,IAA3B,EAH9B;;cAAA;gBAAA;gBAAA,OAKwBpB,EAAE,CAAC2B,IAAH,CAAQjB,IAAR,EAAcU,IAAd,EAAoB;kBAAED,KAAK,EAAElB;gBAAT,CAApB,CALxB;;cAAA;gBAKQ2B,OALR;;gBAAA,MAOM,CAACA,OAAO,CAACC,KAAT,IAAkB,CAACD,OAAO,CAACE,EAPjC;kBAAA;kBAAA;gBAAA;;gBAAA,MAO2ClC,QAAQ,CAACmC,YAAT,EAP3C;;cAAA;gBASE;gBACAN,MAAM,GAAGA,MAAM,IAAI9B,QAAQ,CAACD,aAAD,CAA3B;;gBACMsC,MAXR,GAWiB,SAATA,MAAS,CAAAC,CAAC;kBAAA,OAAIL,OAAO,CAACC,KAAR,CAAcK,QAAd,CAAuBD,CAAC,CAACb,IAAzB,CAAJ;gBAAA,CAXlB;;gBAAA;gBAAA,OAaoB5B,OAAO,CAAC2C,QAAR,CAAiBzB,IAAjB,EAAuBkB,OAAO,CAACC,KAA/B,EAChB;kBAAEvB,MAAM,EAANA,MAAF;kBAAUe,OAAO,EAAPA,OAAV;kBAAmBC,aAAa,EAAbA,aAAnB;kBAAkCC,OAAO,EAAPA,OAAlC;kBAA2CC,WAAW,EAAXA,WAA3C;kBAAwDE,kBAAkB,EAAlBA;gBAAxD,CADgB,CAbpB;;cAAA;gBAaQU,GAbR;gBAgBQC,KAhBR,GAgBgBT,OAAO,CAACE,EAhBxB;gBAiBQQ,OAjBR,GAiBkBhC,MAAM,GAAG,CAAC,CAAV,GAAcJ,IAAI,CAACkC,GAAG,CAACG,IAAJ,CAASd,MAAT,CAAD,EAAmBnB,MAAnB,CAAlB,GAA+C8B,GAjBjE;gBAkBQP,KAlBR,GAkBgBS,OAAO,CAACE,MAAR,CAAeR,MAAf,CAlBhB;gBAAA,kCAmBS;kBAAEK,KAAK,EAALA,KAAF;kBAASC,OAAO,EAAPA,OAAT;kBAAkBT,KAAK,EAALA;gBAAlB,CAnBT;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,C;;;;;;;;IAsBA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;sFACE,kBAA4BnB,IAA5B,EAAkCU,IAAlC;QAAA;;QAAA;UAAA;YAAA;cAAA;gBAAA,qBACId,MADJ,EACIA,MADJ,6BACa,CAAC,CADd,uCACiBe,OADjB,EACiBA,OADjB,8BAC2B,EAD3B,kBAC+BC,aAD/B,SAC+BA,aAD/B,EAC8CC,OAD9C,SAC8CA,OAD9C,EACuDC,WADvD,SACuDA,WADvD,EACoEC,MADpE,SACoEA,MADpE,EAC4EC,kBAD5E,SAC4EA,kBAD5E;;gBAAA,IAEO7B,SAAS,CAACa,IAAD,CAFhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAE8Bd,QAAQ,CAAC6C,mBAAT,EAF9B;;cAAA;gBAAA,IAGO5C,SAAS,CAACuB,IAAD,CAHhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAG8B,IAAIJ,KAAJ,CAAU,wBAAV,CAH9B;;cAAA;gBAIE;gBACM0B,MALR,GAKiBC,KAAK,CAACC,OAAN,CAAcxB,IAAd,IAAsBA,IAAtB,GAA6B,CAACA,IAAD,CAL9C,EAME;;gBACAd,MAAM,GAAGA,MAAM,GAAG,CAAC,CAAV,GAAcC,IAAI,CAACsC,GAAL,CAASvC,MAAT,EAAiB,CAAjB,CAAd,GAAoCA,MAA7C;gBAPF;gBAAA,OAQoBd,OAAO,CAACsD,aAAR,CAAsBpC,IAAtB,EAA4BgC,MAA5B,EAChB;kBAAEpC,MAAM,EAANA,MAAF;kBAAUe,OAAO,EAAPA,OAAV;kBAAmBC,aAAa,EAAbA,aAAnB;kBAAkCC,OAAO,EAAPA,OAAlC;kBAA2CC,WAAW,EAAXA,WAA3C;kBAAwDE,kBAAkB,EAAlBA;gBAAxD,CADgB,CARpB;;cAAA;gBAQQU,GARR;gBAUE;gBACA;gBACAX,MAAM,GAAGA,MAAM,IAAI9B,QAAQ,CAACD,aAAD,CAA3B;gBACM4C,OAbR,GAakBhC,MAAM,GAAG,CAAC,CAAV,GAAcJ,IAAI,CAACkC,GAAG,CAACG,IAAJ,CAASd,MAAT,CAAD,EAAmBnB,MAAnB,CAAlB,GAA+C8B,GAbjE;gBAAA,kCAcS;kBAAEE,OAAO,EAAPA;gBAAF,CAdT;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,C;;;;;;;;IAiBA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;iFACE,kBAAuB5B,IAAvB,EAA6BqC,IAA7B;QAAA;;QAAA;UAAA;YAAA;cAAA;gBAAA,qBAAqCzC,MAArC,EAAqCA,MAArC,6BAA8C,CAAC,CAA/C,iBAAkDiB,OAAlD,SAAkDA,OAAlD,EAA2DC,WAA3D,SAA2DA,WAA3D,EAAwEE,kBAAxE,SAAwEA,kBAAxE;;gBAAA,IACO7B,SAAS,CAACa,IAAD,CADhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAC8Bd,QAAQ,CAACiB,mBAAT,EAD9B;;cAAA;gBAEUiB,EAFV,GAEwBiB,IAFxB,CAEUjB,EAFV,EAEcD,KAFd,GAEwBkB,IAFxB,CAEclB,KAFd;gBAGQmB,UAHR,GAGqBnB,KAAK,CAACoB,GAAN,CAAU,UAAAhB,CAAC;kBAAA,OAAIA,CAAC,CAACb,IAAN;gBAAA,CAAX,CAHrB;gBAAA;gBAAA,OAIoB5B,OAAO,CAACsD,aAAR,CAAsBpC,IAAtB,EAA4BsC,UAA5B,EAChB;kBAAE1C,MAAM,EAANA,MAAF;kBAAUiB,OAAO,EAAPA,OAAV;kBAAmBC,WAAW,EAAXA,WAAnB;kBAAgCE,kBAAkB,EAAlBA;gBAAhC,CADgB,CAJpB;;cAAA;gBAIQU,GAJR;gBAMQE,OANR,GAMkBF,GAAG,CAACG,IAAJ,CAASjD,KAAK,CAAC4D,OAAf,CANlB;gBAAA,kCAOS;kBAAEb,KAAK,EAAEP,EAAT;kBAAaQ,OAAO,EAAPA,OAAb;kBAAsBT,KAAK,EAALA;gBAAtB,CAPT;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,C;;;;;;;;IAUA;AACF;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;kFACE,kBAAwBnB,IAAxB,EAA8ByC,aAA9B;QAAA;;QAAA;UAAA;YAAA;cAAA;gBAAA,qBACI7C,MADJ,EACIA,MADJ,6BACa,CAAC,CADd,uCACiBe,OADjB,EACiBA,OADjB,8BAC2B,EAD3B,kBAC+BC,aAD/B,SAC+BA,aAD/B,EAC8CC,OAD9C,SAC8CA,OAD9C,EACuDC,WADvD,SACuDA,WADvD,EACoEE,kBADpE,SACoEA,kBADpE;;gBAAA,IAEO7B,SAAS,CAACa,IAAD,CAFhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAE8Bd,QAAQ,CAACiB,mBAAT,EAF9B;;cAAA;gBAAA,IAGOhB,SAAS,CAACsD,aAAD,CAHhB;kBAAA;kBAAA;gBAAA;;gBAAA,MAGuC,IAAInC,KAAJ,CAAU,iCAAV,CAHvC;;cAAA;gBAAA,MAMM,CAAC2B,KAAK,CAACC,OAAN,CAAcO,aAAd,CAAD,IAAiC,CAAC7D,KAAK,CAAC8D,OAAN,CAAcD,aAAd,CANxC;kBAAA;kBAAA;gBAAA;;gBAAA,MAOU,IAAInC,KAAJ,CAAU,kFAAV,CAPV;;cAAA;gBAUE,IAAI,CAAC2B,KAAK,CAACC,OAAN,CAAcO,aAAd,CAAL,EAAmC;kBACjCA,aAAa,GAAG,CAACA,aAAD,CAAhB;gBACD,CAZH,CAcE;;;gBACA7C,MAAM,GAAGA,MAAM,GAAG,CAAC,CAAV,GAAcC,IAAI,CAACsC,GAAL,CAASvC,MAAT,EAAiB6C,aAAa,CAAC7C,MAA/B,CAAd,GAAuDA,MAAhE,CAfF,CAiBE;;gBACMoC,MAlBR,GAkBiBS,aAAa,CAACF,GAAd,CAAkB,UAAAhB,CAAC;kBAAA,OAAIA,CAAC,CAACb,IAAN;gBAAA,CAAnB,CAlBjB,EAoBE;;gBApBF;gBAAA,OAqBoB5B,OAAO,CAACsD,aAAR,CAAsBpC,IAAtB,EAA4BgC,MAA5B,EAChB;kBAAEpC,MAAM,EAANA,MAAF;kBAAUe,OAAO,EAAPA,OAAV;kBAAmBC,aAAa,EAAbA,aAAnB;kBAAkCC,OAAO,EAAPA,OAAlC;kBAA2CC,WAAW,EAAXA,WAA3C;kBAAwDE,kBAAkB,EAAlBA;gBAAxD,CADgB,CArBpB;;cAAA;gBAqBQU,GArBR;gBAwBE;gBACMiB,QAzBR,GAyBmBF,aAAa,CAACG,MAAd,CAAqBlB,GAArB,EAA0BkB,MAA1B,CAAiCjC,OAAjC,CAzBnB;gBA0BQkC,OA1BR,GA0BkBzD,WAAW,CAACuD,QAAD,EAAW,MAAX,CAAX,CAA8Bd,IAA9B,CAAmCjD,KAAK,CAAC4D,OAAzC,CA1BlB,EA4BE;;gBACMM,MA7BR,GA6BiBD,OAAO,CAAClD,KAAR,CAAcC,MAAM,GAAG,CAAC,CAAV,GAAc,CAACA,MAAf,GAAwB,CAACiD,OAAO,CAACjD,MAA/C,CA7BjB,EA+BE;gBACA;;gBACMmD,oBAjCR,GAiC+B1D,UAAU,CAACyD,MAAD,EAASL,aAAT,EAAwB,MAAxB,CAjCzC;;gBAmCQO,cAnCR,GAmCyB,SAAjBA,cAAiB,CAACC,CAAD,EAAIC,WAAJ,EAAoB;kBACzC,IAAMJ,MAAM,GAAGG,CAAC,CAACtD,KAAF,CAAQuD,WAAW,CAACtD,MAApB,EAA4BqD,CAAC,CAACrD,MAA9B,CAAf;kBACA,OAAOsD,WAAW,CAACN,MAAZ,CAAmBE,MAAnB,CAAP;gBACD,CAtCH,EAwCE;gBACA;;;gBACMlB,OA1CR,GA0CkBoB,cAAc,CAACF,MAAD,EAASC,oBAAT,CA1ChC;gBA2CQpB,KA3CR,GA2CgBC,OAAO,CAACA,OAAO,CAAChC,MAAR,GAAiB,CAAlB,CAAP,CAA4BwB,EA3C5C;gBAAA,kCA4CS;kBAAEO,KAAK,EAALA,KAAF;kBAASC,OAAO,EAAPA;gBAAT,CA5CT;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,C;;;;;;;;;;;;;AAgDFuB,MAAM,CAACC,OAAP,GAAiBrD,KAAjB"},"metadata":{},"sourceType":"script"}