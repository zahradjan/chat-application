{"ast":null,"code":"'use strict';\n\nconst dagPb = require('@ipld/dag-pb');\n\nconst {\n  Bucket,\n  createHAMT\n} = require('hamt-sharding');\n\nconst DirSharded = require('./dir-sharded');\n\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils');\n\nconst {\n  UnixFS\n} = require('ipfs-unixfs');\n\nconst last = require('it-last');\n\nconst {\n  CID\n} = require('multiformats/cid');\n\nconst {\n  hamtHashCode,\n  hamtHashFn,\n  hamtBucketBits\n} = require('./hamt-constants');\n/**\n * @typedef {import('multiformats/cid').CIDVersion} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\n\n\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  if (!options.parent.Data) {\n    throw new Error('Could not update HAMT directory because parent had no data');\n  } // update parent with new bit field\n\n\n  const data = Uint8Array.from(bucket._children.bitField().reverse());\n  const node = UnixFS.unmarshal(options.parent.Data);\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: hamtHashCode,\n    mode: node.mode,\n    mtime: node.mtime\n  });\n  const hasher = await context.hashers.getHasher(options.hashAlg);\n  const parent = {\n    Data: dir.marshal(),\n    Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n  };\n  const buf = dagPb.encode(parent);\n  const hash = await hasher.digest(buf);\n  const cid = CID.create(options.cidVersion, dagPb.code, hash);\n\n  if (options.flush) {\n    await context.repo.blocks.put(cid, buf);\n  }\n\n  return {\n    node: parent,\n    cid,\n    size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n  };\n};\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\n\n\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hash: rootBucket._options.hash,\n    bits: rootBucket._options.bits\n  }, parentBucket, positionAtParent);\n\n  parentBucket._putObjectAt(positionAtParent, bucket);\n\n  await addLinksToHamtBucket(links, bucket, rootBucket);\n  return bucket;\n};\n/**\n * @param {PBLink[]} links\n */\n\n\nconst recreateInitialHamtLevel = async links => {\n  const bucket = createHAMT({\n    hashFn: hamtHashFn,\n    bits: hamtBucketBits\n  });\n  await addLinksToHamtBucket(links, bucket, bucket);\n  return bucket;\n};\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\n\n\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(links.map(link => {\n    const linkName = link.Name || '';\n\n    if (linkName.length === 2) {\n      const pos = parseInt(linkName, 16);\n\n      bucket._putObjectAt(pos, new Bucket({\n        hash: rootBucket._options.hash,\n        bits: rootBucket._options.bits\n      }, bucket, pos));\n\n      return Promise.resolve();\n    }\n\n    return rootBucket.put(linkName.substring(2), {\n      size: link.Tsize,\n      cid: link.Hash\n    });\n  }));\n};\n/**\n * @param {number} position\n */\n\n\nconst toPrefix = position => {\n  return position.toString(16).toUpperCase().padStart(2, '0').substring(0, 2);\n};\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\n\n\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateInitialHamtLevel(rootNode.Links);\n  const position = await rootBucket._findNewBucketAndPos(fileName); // the path to the root bucket\n\n  /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }];\n  let currentBucket = position.bucket;\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    }); // @ts-ignore - only the root bucket's parent will be undefined\n\n    currentBucket = currentBucket._parent;\n  }\n\n  path.reverse();\n  path[0].node = rootNode; // load PbNode for each path segment\n\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i];\n\n    if (!segment.node) {\n      throw new Error('Could not generate HAMT path');\n    } // find prefix in links\n\n\n    const link = segment.node.Links.filter(link => (link.Name || '').substring(0, 2) === segment.prefix).pop(); // entry was not in shard\n\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`); // return path\n\n      continue;\n    } // found entry\n\n\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`); // file already existed, file will be added to the current bucket\n      // return path\n\n      continue;\n    } // found subshard\n\n\n    log(`Found subshard ${segment.prefix}`);\n    const block = await context.repo.blocks.get(link.Hash);\n    const node = dagPb.decode(block); // subshard hasn't been loaded, descend to the next level of the HAMT\n\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`);\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16));\n      const position = await rootBucket._findNewBucketAndPos(fileName); // i--\n\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      });\n      continue;\n    }\n\n    const nextSegment = path[i + 1]; // add intermediate links to bucket\n\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket);\n    nextSegment.node = node;\n  }\n\n  await rootBucket.put(fileName, true);\n  path.reverse();\n  return {\n    rootBucket,\n    path\n  };\n};\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\n\n\nconst createShard = async function (context, contents) {\n  let options = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: undefined,\n    parentKey: undefined,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, options);\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    });\n  }\n\n  const res = await last(shard.flush(context.repo.blocks));\n\n  if (!res) {\n    throw new Error('Flushing shard yielded no result');\n  }\n\n  return res;\n};\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  recreateInitialHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n};","map":{"version":3,"names":["dagPb","require","Bucket","createHAMT","DirSharded","log","UnixFS","last","CID","hamtHashCode","hamtHashFn","hamtBucketBits","updateHamtDirectory","context","links","bucket","options","parent","Data","Error","data","Uint8Array","from","_children","bitField","reverse","node","unmarshal","dir","type","fanout","tableSize","hashType","mode","mtime","hasher","hashers","getHasher","hashAlg","marshal","Links","sort","a","b","Name","localeCompare","buf","encode","hash","digest","cid","create","cidVersion","code","flush","repo","blocks","put","size","reduce","sum","link","Tsize","length","recreateHamtLevel","rootBucket","parentBucket","positionAtParent","_options","bits","_putObjectAt","addLinksToHamtBucket","recreateInitialHamtLevel","hashFn","Promise","all","map","linkName","pos","parseInt","resolve","substring","Hash","toPrefix","position","toString","toUpperCase","padStart","generatePath","fileName","rootNode","_findNewBucketAndPos","path","prefix","currentBucket","push","_posAtParent","_parent","i","segment","filter","pop","block","get","decode","nextSegment","createShard","contents","shard","root","undefined","parentKey","dirty","flat","_bucket","name","res","module","exports"],"sources":["C:/Users/zajan/GitHub/chatApplication/node_modules/ipfs-core/src/components/files/utils/hamt-utils.js"],"sourcesContent":["'use strict'\n\nconst dagPb = require('@ipld/dag-pb')\nconst {\n  Bucket,\n  createHAMT\n} = require('hamt-sharding')\nconst DirSharded = require('./dir-sharded')\nconst log = require('debug')('ipfs:mfs:core:utils:hamt-utils')\nconst { UnixFS } = require('ipfs-unixfs')\nconst last = require('it-last')\nconst { CID } = require('multiformats/cid')\nconst {\n  hamtHashCode,\n  hamtHashFn,\n  hamtBucketBits\n} = require('./hamt-constants')\n\n/**\n * @typedef {import('multiformats/cid').CIDVersion} CIDVersion\n * @typedef {import('ipfs-unixfs').Mtime} Mtime\n * @typedef {import('../').MfsContext} MfsContext\n * @typedef {import('@ipld/dag-pb').PBNode} PBNode\n * @typedef {import('@ipld/dag-pb').PBLink} PBLink\n */\n\n/**\n * @param {MfsContext} context\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {object} options\n * @param {PBNode} options.parent\n * @param {CIDVersion} options.cidVersion\n * @param {boolean} options.flush\n * @param {string} options.hashAlg\n */\nconst updateHamtDirectory = async (context, links, bucket, options) => {\n  if (!options.parent.Data) {\n    throw new Error('Could not update HAMT directory because parent had no data')\n  }\n\n  // update parent with new bit field\n  const data = Uint8Array.from(bucket._children.bitField().reverse())\n  const node = UnixFS.unmarshal(options.parent.Data)\n  const dir = new UnixFS({\n    type: 'hamt-sharded-directory',\n    data,\n    fanout: bucket.tableSize(),\n    hashType: hamtHashCode,\n    mode: node.mode,\n    mtime: node.mtime\n  })\n\n  const hasher = await context.hashers.getHasher(options.hashAlg)\n  const parent = {\n    Data: dir.marshal(),\n    Links: links.sort((a, b) => (a.Name || '').localeCompare(b.Name || ''))\n  }\n  const buf = dagPb.encode(parent)\n  const hash = await hasher.digest(buf)\n  const cid = CID.create(options.cidVersion, dagPb.code, hash)\n\n  if (options.flush) {\n    await context.repo.blocks.put(cid, buf)\n  }\n\n  return {\n    node: parent,\n    cid,\n    size: links.reduce((sum, link) => sum + (link.Tsize || 0), buf.length)\n  }\n}\n\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} rootBucket\n * @param {Bucket<any>} parentBucket\n * @param {number} positionAtParent\n */\nconst recreateHamtLevel = async (links, rootBucket, parentBucket, positionAtParent) => {\n  // recreate this level of the HAMT\n  const bucket = new Bucket({\n    hash: rootBucket._options.hash,\n    bits: rootBucket._options.bits\n  }, parentBucket, positionAtParent)\n  parentBucket._putObjectAt(positionAtParent, bucket)\n\n  await addLinksToHamtBucket(links, bucket, rootBucket)\n\n  return bucket\n}\n\n/**\n * @param {PBLink[]} links\n */\nconst recreateInitialHamtLevel = async (links) => {\n  const bucket = createHAMT({\n    hashFn: hamtHashFn,\n    bits: hamtBucketBits\n  })\n\n  await addLinksToHamtBucket(links, bucket, bucket)\n\n  return bucket\n}\n\n/**\n * @param {PBLink[]} links\n * @param {Bucket<any>} bucket\n * @param {Bucket<any>} rootBucket\n */\nconst addLinksToHamtBucket = async (links, bucket, rootBucket) => {\n  await Promise.all(\n    links.map(link => {\n      const linkName = (link.Name || '')\n\n      if (linkName.length === 2) {\n        const pos = parseInt(linkName, 16)\n\n        bucket._putObjectAt(pos, new Bucket({\n          hash: rootBucket._options.hash,\n          bits: rootBucket._options.bits\n        }, bucket, pos))\n\n        return Promise.resolve()\n      }\n\n      return rootBucket.put(linkName.substring(2), {\n        size: link.Tsize,\n        cid: link.Hash\n      })\n    })\n  )\n}\n\n/**\n * @param {number} position\n */\nconst toPrefix = (position) => {\n  return position\n    .toString(16)\n    .toUpperCase()\n    .padStart(2, '0')\n    .substring(0, 2)\n}\n\n/**\n * @param {MfsContext} context\n * @param {string} fileName\n * @param {PBNode} rootNode\n */\nconst generatePath = async (context, fileName, rootNode) => {\n  // start at the root bucket and descend, loading nodes as we go\n  const rootBucket = await recreateInitialHamtLevel(rootNode.Links)\n  const position = await rootBucket._findNewBucketAndPos(fileName)\n\n  // the path to the root bucket\n  /** @type {{ bucket: Bucket<any>, prefix: string, node?: PBNode }[]} */\n  const path = [{\n    bucket: position.bucket,\n    prefix: toPrefix(position.pos)\n  }]\n  let currentBucket = position.bucket\n\n  while (currentBucket !== rootBucket) {\n    path.push({\n      bucket: currentBucket,\n      prefix: toPrefix(currentBucket._posAtParent)\n    })\n\n    // @ts-ignore - only the root bucket's parent will be undefined\n    currentBucket = currentBucket._parent\n  }\n\n  path.reverse()\n  path[0].node = rootNode\n\n  // load PbNode for each path segment\n  for (let i = 0; i < path.length; i++) {\n    const segment = path[i]\n\n    if (!segment.node) {\n      throw new Error('Could not generate HAMT path')\n    }\n\n    // find prefix in links\n    const link = segment.node.Links\n      .filter(link => (link.Name || '').substring(0, 2) === segment.prefix)\n      .pop()\n\n    // entry was not in shard\n    if (!link) {\n      // reached bottom of tree, file will be added to the current bucket\n      log(`Link ${segment.prefix}${fileName} will be added`)\n      // return path\n      continue\n    }\n\n    // found entry\n    if (link.Name === `${segment.prefix}${fileName}`) {\n      log(`Link ${segment.prefix}${fileName} will be replaced`)\n      // file already existed, file will be added to the current bucket\n      // return path\n      continue\n    }\n\n    // found subshard\n    log(`Found subshard ${segment.prefix}`)\n    const block = await context.repo.blocks.get(link.Hash)\n    const node = dagPb.decode(block)\n\n    // subshard hasn't been loaded, descend to the next level of the HAMT\n    if (!path[i + 1]) {\n      log(`Loaded new subshard ${segment.prefix}`)\n\n      await recreateHamtLevel(node.Links, rootBucket, segment.bucket, parseInt(segment.prefix, 16))\n      const position = await rootBucket._findNewBucketAndPos(fileName)\n\n      // i--\n      path.push({\n        bucket: position.bucket,\n        prefix: toPrefix(position.pos),\n        node: node\n      })\n\n      continue\n    }\n\n    const nextSegment = path[i + 1]\n\n    // add intermediate links to bucket\n    await addLinksToHamtBucket(node.Links, nextSegment.bucket, rootBucket)\n\n    nextSegment.node = node\n  }\n\n  await rootBucket.put(fileName, true)\n\n  path.reverse()\n\n  return {\n    rootBucket,\n    path\n  }\n}\n\n/**\n * @param {MfsContext} context\n * @param {{ name: string, size: number, cid: CID }[]} contents\n * @param {object} [options]\n * @param {Mtime} [options.mtime]\n * @param {number} [options.mode]\n */\nconst createShard = async (context, contents, options = {}) => {\n  const shard = new DirSharded({\n    root: true,\n    dir: true,\n    parent: undefined,\n    parentKey: undefined,\n    path: '',\n    dirty: true,\n    flat: false,\n    mtime: options.mtime,\n    mode: options.mode\n  }, options)\n\n  for (let i = 0; i < contents.length; i++) {\n    await shard._bucket.put(contents[i].name, {\n      size: contents[i].size,\n      cid: contents[i].cid\n    })\n  }\n\n  const res = await last(shard.flush(context.repo.blocks))\n\n  if (!res) {\n    throw new Error('Flushing shard yielded no result')\n  }\n\n  return res\n}\n\nmodule.exports = {\n  generatePath,\n  updateHamtDirectory,\n  recreateHamtLevel,\n  recreateInitialHamtLevel,\n  addLinksToHamtBucket,\n  toPrefix,\n  createShard\n}\n"],"mappings":"AAAA;;AAEA,MAAMA,KAAK,GAAGC,OAAO,CAAC,cAAD,CAArB;;AACA,MAAM;EACJC,MADI;EAEJC;AAFI,IAGFF,OAAO,CAAC,eAAD,CAHX;;AAIA,MAAMG,UAAU,GAAGH,OAAO,CAAC,eAAD,CAA1B;;AACA,MAAMI,GAAG,GAAGJ,OAAO,CAAC,OAAD,CAAP,CAAiB,gCAAjB,CAAZ;;AACA,MAAM;EAAEK;AAAF,IAAaL,OAAO,CAAC,aAAD,CAA1B;;AACA,MAAMM,IAAI,GAAGN,OAAO,CAAC,SAAD,CAApB;;AACA,MAAM;EAAEO;AAAF,IAAUP,OAAO,CAAC,kBAAD,CAAvB;;AACA,MAAM;EACJQ,YADI;EAEJC,UAFI;EAGJC;AAHI,IAIFV,OAAO,CAAC,kBAAD,CAJX;AAMA;AACA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMW,mBAAmB,GAAG,OAAOC,OAAP,EAAgBC,KAAhB,EAAuBC,MAAvB,EAA+BC,OAA/B,KAA2C;EACrE,IAAI,CAACA,OAAO,CAACC,MAAR,CAAeC,IAApB,EAA0B;IACxB,MAAM,IAAIC,KAAJ,CAAU,4DAAV,CAAN;EACD,CAHoE,CAKrE;;;EACA,MAAMC,IAAI,GAAGC,UAAU,CAACC,IAAX,CAAgBP,MAAM,CAACQ,SAAP,CAAiBC,QAAjB,GAA4BC,OAA5B,EAAhB,CAAb;EACA,MAAMC,IAAI,GAAGpB,MAAM,CAACqB,SAAP,CAAiBX,OAAO,CAACC,MAAR,CAAeC,IAAhC,CAAb;EACA,MAAMU,GAAG,GAAG,IAAItB,MAAJ,CAAW;IACrBuB,IAAI,EAAE,wBADe;IAErBT,IAFqB;IAGrBU,MAAM,EAAEf,MAAM,CAACgB,SAAP,EAHa;IAIrBC,QAAQ,EAAEvB,YAJW;IAKrBwB,IAAI,EAAEP,IAAI,CAACO,IALU;IAMrBC,KAAK,EAAER,IAAI,CAACQ;EANS,CAAX,CAAZ;EASA,MAAMC,MAAM,GAAG,MAAMtB,OAAO,CAACuB,OAAR,CAAgBC,SAAhB,CAA0BrB,OAAO,CAACsB,OAAlC,CAArB;EACA,MAAMrB,MAAM,GAAG;IACbC,IAAI,EAAEU,GAAG,CAACW,OAAJ,EADO;IAEbC,KAAK,EAAE1B,KAAK,CAAC2B,IAAN,CAAW,CAACC,CAAD,EAAIC,CAAJ,KAAU,CAACD,CAAC,CAACE,IAAF,IAAU,EAAX,EAAeC,aAAf,CAA6BF,CAAC,CAACC,IAAF,IAAU,EAAvC,CAArB;EAFM,CAAf;EAIA,MAAME,GAAG,GAAG9C,KAAK,CAAC+C,MAAN,CAAa9B,MAAb,CAAZ;EACA,MAAM+B,IAAI,GAAG,MAAMb,MAAM,CAACc,MAAP,CAAcH,GAAd,CAAnB;EACA,MAAMI,GAAG,GAAG1C,GAAG,CAAC2C,MAAJ,CAAWnC,OAAO,CAACoC,UAAnB,EAA+BpD,KAAK,CAACqD,IAArC,EAA2CL,IAA3C,CAAZ;;EAEA,IAAIhC,OAAO,CAACsC,KAAZ,EAAmB;IACjB,MAAMzC,OAAO,CAAC0C,IAAR,CAAaC,MAAb,CAAoBC,GAApB,CAAwBP,GAAxB,EAA6BJ,GAA7B,CAAN;EACD;;EAED,OAAO;IACLpB,IAAI,EAAET,MADD;IAELiC,GAFK;IAGLQ,IAAI,EAAE5C,KAAK,CAAC6C,MAAN,CAAa,CAACC,GAAD,EAAMC,IAAN,KAAeD,GAAG,IAAIC,IAAI,CAACC,KAAL,IAAc,CAAlB,CAA/B,EAAqDhB,GAAG,CAACiB,MAAzD;EAHD,CAAP;AAKD,CAnCD;AAqCA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMC,iBAAiB,GAAG,OAAOlD,KAAP,EAAcmD,UAAd,EAA0BC,YAA1B,EAAwCC,gBAAxC,KAA6D;EACrF;EACA,MAAMpD,MAAM,GAAG,IAAIb,MAAJ,CAAW;IACxB8C,IAAI,EAAEiB,UAAU,CAACG,QAAX,CAAoBpB,IADF;IAExBqB,IAAI,EAAEJ,UAAU,CAACG,QAAX,CAAoBC;EAFF,CAAX,EAGZH,YAHY,EAGEC,gBAHF,CAAf;;EAIAD,YAAY,CAACI,YAAb,CAA0BH,gBAA1B,EAA4CpD,MAA5C;;EAEA,MAAMwD,oBAAoB,CAACzD,KAAD,EAAQC,MAAR,EAAgBkD,UAAhB,CAA1B;EAEA,OAAOlD,MAAP;AACD,CAXD;AAaA;AACA;AACA;;;AACA,MAAMyD,wBAAwB,GAAG,MAAO1D,KAAP,IAAiB;EAChD,MAAMC,MAAM,GAAGZ,UAAU,CAAC;IACxBsE,MAAM,EAAE/D,UADgB;IAExB2D,IAAI,EAAE1D;EAFkB,CAAD,CAAzB;EAKA,MAAM4D,oBAAoB,CAACzD,KAAD,EAAQC,MAAR,EAAgBA,MAAhB,CAA1B;EAEA,OAAOA,MAAP;AACD,CATD;AAWA;AACA;AACA;AACA;AACA;;;AACA,MAAMwD,oBAAoB,GAAG,OAAOzD,KAAP,EAAcC,MAAd,EAAsBkD,UAAtB,KAAqC;EAChE,MAAMS,OAAO,CAACC,GAAR,CACJ7D,KAAK,CAAC8D,GAAN,CAAUf,IAAI,IAAI;IAChB,MAAMgB,QAAQ,GAAIhB,IAAI,CAACjB,IAAL,IAAa,EAA/B;;IAEA,IAAIiC,QAAQ,CAACd,MAAT,KAAoB,CAAxB,EAA2B;MACzB,MAAMe,GAAG,GAAGC,QAAQ,CAACF,QAAD,EAAW,EAAX,CAApB;;MAEA9D,MAAM,CAACuD,YAAP,CAAoBQ,GAApB,EAAyB,IAAI5E,MAAJ,CAAW;QAClC8C,IAAI,EAAEiB,UAAU,CAACG,QAAX,CAAoBpB,IADQ;QAElCqB,IAAI,EAAEJ,UAAU,CAACG,QAAX,CAAoBC;MAFQ,CAAX,EAGtBtD,MAHsB,EAGd+D,GAHc,CAAzB;;MAKA,OAAOJ,OAAO,CAACM,OAAR,EAAP;IACD;;IAED,OAAOf,UAAU,CAACR,GAAX,CAAeoB,QAAQ,CAACI,SAAT,CAAmB,CAAnB,CAAf,EAAsC;MAC3CvB,IAAI,EAAEG,IAAI,CAACC,KADgC;MAE3CZ,GAAG,EAAEW,IAAI,CAACqB;IAFiC,CAAtC,CAAP;EAID,CAlBD,CADI,CAAN;AAqBD,CAtBD;AAwBA;AACA;AACA;;;AACA,MAAMC,QAAQ,GAAIC,QAAD,IAAc;EAC7B,OAAOA,QAAQ,CACZC,QADI,CACK,EADL,EAEJC,WAFI,GAGJC,QAHI,CAGK,CAHL,EAGQ,GAHR,EAIJN,SAJI,CAIM,CAJN,EAIS,CAJT,CAAP;AAKD,CAND;AAQA;AACA;AACA;AACA;AACA;;;AACA,MAAMO,YAAY,GAAG,OAAO3E,OAAP,EAAgB4E,QAAhB,EAA0BC,QAA1B,KAAuC;EAC1D;EACA,MAAMzB,UAAU,GAAG,MAAMO,wBAAwB,CAACkB,QAAQ,CAAClD,KAAV,CAAjD;EACA,MAAM4C,QAAQ,GAAG,MAAMnB,UAAU,CAAC0B,oBAAX,CAAgCF,QAAhC,CAAvB,CAH0D,CAK1D;;EACA;;EACA,MAAMG,IAAI,GAAG,CAAC;IACZ7E,MAAM,EAAEqE,QAAQ,CAACrE,MADL;IAEZ8E,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACN,GAAV;EAFJ,CAAD,CAAb;EAIA,IAAIgB,aAAa,GAAGV,QAAQ,CAACrE,MAA7B;;EAEA,OAAO+E,aAAa,KAAK7B,UAAzB,EAAqC;IACnC2B,IAAI,CAACG,IAAL,CAAU;MACRhF,MAAM,EAAE+E,aADA;MAERD,MAAM,EAAEV,QAAQ,CAACW,aAAa,CAACE,YAAf;IAFR,CAAV,EADmC,CAMnC;;IACAF,aAAa,GAAGA,aAAa,CAACG,OAA9B;EACD;;EAEDL,IAAI,CAACnE,OAAL;EACAmE,IAAI,CAAC,CAAD,CAAJ,CAAQlE,IAAR,GAAegE,QAAf,CAxB0D,CA0B1D;;EACA,KAAK,IAAIQ,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGN,IAAI,CAAC7B,MAAzB,EAAiCmC,CAAC,EAAlC,EAAsC;IACpC,MAAMC,OAAO,GAAGP,IAAI,CAACM,CAAD,CAApB;;IAEA,IAAI,CAACC,OAAO,CAACzE,IAAb,EAAmB;MACjB,MAAM,IAAIP,KAAJ,CAAU,8BAAV,CAAN;IACD,CALmC,CAOpC;;;IACA,MAAM0C,IAAI,GAAGsC,OAAO,CAACzE,IAAR,CAAac,KAAb,CACV4D,MADU,CACHvC,IAAI,IAAI,CAACA,IAAI,CAACjB,IAAL,IAAa,EAAd,EAAkBqC,SAAlB,CAA4B,CAA5B,EAA+B,CAA/B,MAAsCkB,OAAO,CAACN,MADnD,EAEVQ,GAFU,EAAb,CARoC,CAYpC;;IACA,IAAI,CAACxC,IAAL,EAAW;MACT;MACAxD,GAAG,CAAE,QAAO8F,OAAO,CAACN,MAAO,GAAEJ,QAAS,gBAAnC,CAAH,CAFS,CAGT;;MACA;IACD,CAlBmC,CAoBpC;;;IACA,IAAI5B,IAAI,CAACjB,IAAL,KAAe,GAAEuD,OAAO,CAACN,MAAO,GAAEJ,QAAS,EAA/C,EAAkD;MAChDpF,GAAG,CAAE,QAAO8F,OAAO,CAACN,MAAO,GAAEJ,QAAS,mBAAnC,CAAH,CADgD,CAEhD;MACA;;MACA;IACD,CA1BmC,CA4BpC;;;IACApF,GAAG,CAAE,kBAAiB8F,OAAO,CAACN,MAAO,EAAlC,CAAH;IACA,MAAMS,KAAK,GAAG,MAAMzF,OAAO,CAAC0C,IAAR,CAAaC,MAAb,CAAoB+C,GAApB,CAAwB1C,IAAI,CAACqB,IAA7B,CAApB;IACA,MAAMxD,IAAI,GAAG1B,KAAK,CAACwG,MAAN,CAAaF,KAAb,CAAb,CA/BoC,CAiCpC;;IACA,IAAI,CAACV,IAAI,CAACM,CAAC,GAAG,CAAL,CAAT,EAAkB;MAChB7F,GAAG,CAAE,uBAAsB8F,OAAO,CAACN,MAAO,EAAvC,CAAH;MAEA,MAAM7B,iBAAiB,CAACtC,IAAI,CAACc,KAAN,EAAayB,UAAb,EAAyBkC,OAAO,CAACpF,MAAjC,EAAyCgE,QAAQ,CAACoB,OAAO,CAACN,MAAT,EAAiB,EAAjB,CAAjD,CAAvB;MACA,MAAMT,QAAQ,GAAG,MAAMnB,UAAU,CAAC0B,oBAAX,CAAgCF,QAAhC,CAAvB,CAJgB,CAMhB;;MACAG,IAAI,CAACG,IAAL,CAAU;QACRhF,MAAM,EAAEqE,QAAQ,CAACrE,MADT;QAER8E,MAAM,EAAEV,QAAQ,CAACC,QAAQ,CAACN,GAAV,CAFR;QAGRpD,IAAI,EAAEA;MAHE,CAAV;MAMA;IACD;;IAED,MAAM+E,WAAW,GAAGb,IAAI,CAACM,CAAC,GAAG,CAAL,CAAxB,CAlDoC,CAoDpC;;IACA,MAAM3B,oBAAoB,CAAC7C,IAAI,CAACc,KAAN,EAAaiE,WAAW,CAAC1F,MAAzB,EAAiCkD,UAAjC,CAA1B;IAEAwC,WAAW,CAAC/E,IAAZ,GAAmBA,IAAnB;EACD;;EAED,MAAMuC,UAAU,CAACR,GAAX,CAAegC,QAAf,EAAyB,IAAzB,CAAN;EAEAG,IAAI,CAACnE,OAAL;EAEA,OAAO;IACLwC,UADK;IAEL2B;EAFK,CAAP;AAID,CA7FD;AA+FA;AACA;AACA;AACA;AACA;AACA;AACA;;;AACA,MAAMc,WAAW,GAAG,gBAAO7F,OAAP,EAAgB8F,QAAhB,EAA2C;EAAA,IAAjB3F,OAAiB,uEAAP,EAAO;EAC7D,MAAM4F,KAAK,GAAG,IAAIxG,UAAJ,CAAe;IAC3ByG,IAAI,EAAE,IADqB;IAE3BjF,GAAG,EAAE,IAFsB;IAG3BX,MAAM,EAAE6F,SAHmB;IAI3BC,SAAS,EAAED,SAJgB;IAK3BlB,IAAI,EAAE,EALqB;IAM3BoB,KAAK,EAAE,IANoB;IAO3BC,IAAI,EAAE,KAPqB;IAQ3B/E,KAAK,EAAElB,OAAO,CAACkB,KARY;IAS3BD,IAAI,EAAEjB,OAAO,CAACiB;EATa,CAAf,EAUXjB,OAVW,CAAd;;EAYA,KAAK,IAAIkF,CAAC,GAAG,CAAb,EAAgBA,CAAC,GAAGS,QAAQ,CAAC5C,MAA7B,EAAqCmC,CAAC,EAAtC,EAA0C;IACxC,MAAMU,KAAK,CAACM,OAAN,CAAczD,GAAd,CAAkBkD,QAAQ,CAACT,CAAD,CAAR,CAAYiB,IAA9B,EAAoC;MACxCzD,IAAI,EAAEiD,QAAQ,CAACT,CAAD,CAAR,CAAYxC,IADsB;MAExCR,GAAG,EAAEyD,QAAQ,CAACT,CAAD,CAAR,CAAYhD;IAFuB,CAApC,CAAN;EAID;;EAED,MAAMkE,GAAG,GAAG,MAAM7G,IAAI,CAACqG,KAAK,CAACtD,KAAN,CAAYzC,OAAO,CAAC0C,IAAR,CAAaC,MAAzB,CAAD,CAAtB;;EAEA,IAAI,CAAC4D,GAAL,EAAU;IACR,MAAM,IAAIjG,KAAJ,CAAU,kCAAV,CAAN;EACD;;EAED,OAAOiG,GAAP;AACD,CA3BD;;AA6BAC,MAAM,CAACC,OAAP,GAAiB;EACf9B,YADe;EAEf5E,mBAFe;EAGfoD,iBAHe;EAIfQ,wBAJe;EAKfD,oBALe;EAMfY,QANe;EAOfuB;AAPe,CAAjB"},"metadata":{},"sourceType":"script"}